---
title: "auto_rate 1: Automatic detection of metabolic rates"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{auto_rate1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = F, warning = F, message = F}
library(knitr) # load knitr to enable options
library(respR) # load respR
# library(dplyr) # for some piping exercises
library(DiagrammeR) # flowcharts... and more

opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE,
  highlight = TRUE, fig.width = 7.1, fig.height = 6)
```

## Introduction

In respirometry, we often want to report oxygen uptake rates that represent experimentally important stages or physiological states. These include:

- the most consistent (i.e. most linear) rates observed, often most representative of *routine* metabolic rates. 
- minimum rates observed, often most representative of *standard*, *resting* or *maintenance* metabolic rates.
- maximum rates observed, representative of *active* or *maximal* metabolic rates.

The problem is that over long datasets identifying these regions is difficult, and if selected visually are subject to bias and lack of observer objectivity. Other methods, such as fitting multiple, fixed-width linear regressions over the entire dataset to identify regions of minimum or maximum slopes is computationally intensive, and slopes found via this method highly sensitive to the width chosen, especially if the specimen's metabolic rate may change rapidly. 

Here we present `auto_rate()`, a function in the `respR` package that uses rolling regression techniques to automatically detect **most linear**, **maximum** and **minimum** rates within a dataset. This function can be used to select linear, max and min rates in an statistically robust, objective way. In this vignette we detail how `auto_rate()` works, and how it can be used to extract rates from respirometry data. In the **LINK** vignette, we show how we have tested this method against simulated data, and the **LINK** vignette its performance against another linear detection method. We show it perfoms extremely well on large datasets in determining linear rates. Importantly, `auto_rate()` has been optimised to be extremely fast. Other methods on large datasets can take minutes or hours to run. We show how `auto_rate()` can reduce this wait by orders of magnitude, literally fitting tens of thousands of regressions and detecting linear regions in seconds. 

## Overview

This illustrates the main processes involved in `auto_rate()`:

```{r, echo = F}
library(DiagrammeR)
grViz("
digraph a_nice_graph {

# node definitions with substituted label text

node [fontname = Helvetica, shape = circle]
rd [label = 'Raw Data']

node [fontname = Helvetica, shape = rectangle]
rr [label = '1. Rolling linear regression']
kd [label = '2. Kernel density\nestimates']
re [label = '3. Bandwidth resampling']
lr [label = '4. Linear regression']
ar [label = '5. Rank']
fi [label = '6. Filter']

node [fontname = Menlo, shape = none]
au [label = 'auto_rate()']
li [label = 'linear,']
mm [label = 'max, min']
in [label = 'interval']

# edge definitions with the node IDs

rd -> au -> rr -> ar -> mm
rr -> kd -> re -> lr -> ar -> li
rr -> fi -> in

}")
```


`auto_rate()` works by performing an optimised rolling regression on the dataset. A kernel density estimate is performed on the rolling regression output and the kernel bandwidth used to re-sample linear regions of the data for re-analysis. 

## Rolling linear regression

The function `auto_rate()` uses a novel method of combining rolling regression and kernel density estimate algorithms to detect patterns in time series data. The rolling regression runs all possible ordinary least-squares (OLS) linear regressions $(y = \beta_0 + \beta_1 X + \epsilon)$ of a fixed sample width across the dataset, and is expressed as: $$y_t(n) = X_t(n) \beta (n) + \epsilon_t(n), \ t = n,\ ...,\ T$$ where $T$ is the total length of the dataset, $n$ is the window of width $n < T$, $y_t(n)$ is the vector of observations (e.g. oxygen concentration), $X_t(n)$ is the matrix of explanatory variables, $\beta (n)$ is a vector of regression parameters and $\epsilon_t(n)$ is a vector of error terms.
Thus, a total of $(T - n) + 1$ number of overlapping regressions are fit. These are then ranked to obtain maximum and minimum values.
An interval-based rolling regression can be selected (`method = "interval"`), and it automatically selects non-overlapping sections of the data for regressions, but all other methods fit overlapping regressions. 

## Kernel density estimation 

<!-- Additional methods are automatically applied when we use `auto_rate()` to detect linear sections of the data.  -->
<!-- First, a Gaussian kernel density estimate (KDE) is used to process the regression coefficient $\beta$, based on the key assumption that linear sections of the data are reflected by relatively stable parameters across the rolling estimates. -->
<!-- This assumption is used often in financial statistics to evaluate stability and make predictions on time-series data. -->
<!-- Here, KDE automatically aggregates stable (i.e. linear) sections as they naturally form one or more local maximums ("modes") in the probability density estimate, and if the distribution is multimodal, the modes are ranked by size. -->

Additional analytical techniques are automatically applied when we use `auto_rate()` to detect linear sections of the data. 
First, we take advantage of the key assumption that linear sections of a data series are reflected by stable parameters across the rolling estimates, a property that is often applied in financial statistics to evaluate model stability and make forward predictions on time-series data (see Zivot and Wang 2006).
We use kernel density estimation (KDE) techniques, often applied in various inference procedures such as machine learning, pattern recognition and computer vision, to automatically aggregate stable (i.e. linear) segments as they naturally form one or more local maximums ("modes") in the probability density estimate.

KDE requires no assumption that the data is from a parametric family, and learns the shape of the density automatically without supervision. 
KDE can be expressed as: $$\hat{f}(x) = \frac{1}{nh^d}\sum_{i = 1}^{n} K \left(\frac{x - X_i}{h} \right)$$ where $f$ is the density function from an unknown distribution $P$ for $X_1,...,X_n$, $K$ is the kernel function and $h$ is the optimal smoothing bandwidth. 
The smoothing bandwidth is computed using the solve-the-equation *plug-in* method (Sheather et al. 1996, Sheather and Jones 1991) which works well with multimodal or non-normal densities (Raykar and Duraiswami 2006).

<!-- The smoothing bandwidth $h$ is computed using an adjusted kernel bandwidth selector based on Silvermans "rule of thumb" (Silverman, 1986): $$h = \left(\frac{4\hat{\sigma}^5}{3n} \right)^{\frac{1}{5}} \approx 1.06\hat{\sigma}n^{-\frac{1}{5}}$$ where $\hat{\sigma}$ is the standard deviation of the samples and $n$ is the total number of samples.  -->

We then use $h$ to select all values in the rolling regression output that match the range of values around each mode ($\theta_n$) of the KDE (i.e. $\theta_n \pm h$).
These rolling estimates are grouped and ranked by size, and the upper and lower bounds of the data windows they represent are used to re-select the linear segment of the original data series. 
The rolling estimates are then discarded while the detected data segments are analysed using linear regression. 

## Adjusting the width of rolling regressions

By default, `auto_rate()`'s rolling regression uses a rolling window value in rows of `0.2` multiplied by the total length of the number of samples. 
This can be changed by changing the `width` argument, to a proportion relative to the total size of the data (`width = 0.1, by = "row"`) or a fixed value in the time metric (`width = 3000, by = "time"`). 

It is important to note that the width determines the exact width of the data segments produced for `max`, `min` and `interva`l rates. This allows the user to consistently report results across experiments, such as reporting the maximum or minimum rates sustained over a specified time period. 

Importantly, the `width` does *not* restrict the maximum width of the segments produced for linear detection. 
**We advise users to use caution when changing the `width` argument if using `method = "linear"`.** Choosing an inappropriate width value tends to over-fit the data for rolling regression. 

Below, we show the differences in the shape of the rolling regressions when using a `width` of `0.6` to analyse `sardine.rd`:

```{r rollreg_demo}
# Perform linear detection; default width (when not specified) is 0.2:
normx <- auto_rate(sardine.rd, plot = FALSE)

# Perform linear detection using manual width of 0.6:
overx <- auto_rate(sardine.rd, plot = FALSE, width = .6)

# Plot ONLY the rolling regression plots for comparison:
par(mfrow = c(1, 2), mai = c(0.4, 0.4, 0.3, 0.3), ps = 10,
    cex = 1, cex.main = 1)
plot(normx, choose = 3)
plot(overx, choose = 3)
```

Under perfectly linear conditions, we would expect the rolling regression output to produce a straight, horizontal line with a slope of 0, i.e. a very stable plot.
Since KDE automatically aggregates stable values, a poor selection of the `width` value may result in a badly-characterised rolling estimate output.
In the example presented above, while the default width showed a pattern of relative stability after 3,000 seconds, that information was lost when a `width` of `0.6` was used.
Thus, the KDE technique would not have had the appropriate information necessary to correctly detect any linear patterns in the same data.

## Most linear rates
By default, `auto_rate()` identifies the *most linear* region of the data (i.e. `method = "linear"`):

```{r}
x <- urchins.rd[,c(1,15)]
auto_rate(x)
```

This represents the most consistently linear region, that is most consistent rate observed during the experiment. It does this in a statistically rigorous manner, which removes observer subjectivity from choosing which rate is most appropriate to report in their results. It calculates these *most linear* rates across all possible data window widths, so this also removes the need to the user to specify this. It is a statistically robust way of indentifying and reporting consistent rates in respirometry data. 

## Minimum and maximum rates

`auto_rate()` can also be used to detect the maximum and minimum rates over a fixed sample width, either as proportion of the dataset (e.g. the default `width = 0.2, by = "row"`), or a fixed period of time (e.g. `width = 600, by = "time"`). This allows for consistent reporting of respirometry results, such as the maximum or minimum rates sustained over a specified time period. Note, that while still fast in comparison to some other methods, this second method of specifying a `time` window can be considerably slower. This is because it is telling `auto_rate()` that the time data may have gaps or not be evenly spaced, and so the function calculates each (in this example) 600 point width using the raw time *values*, rather than assuming a specific row width represents the same time window. If your data is without gaps and evenly spaced with regards to time, this option is not really necessary, and `by = row` and the correct proportional `width` for the time you want should be used, as it is much faster. 

##### Maximum rates:

Here we want to know the maximum rates sustained over 10 minutes, or 600s, in the `sardine.rd` data. Since in these data, O2 data is recorded every second  and `inspect_data()` tells us the time data is gapless and evenly spaced, we can simply specify width in the correct number of rows:

```{r fig.keep='none'}
inspect(sardine.rd)
```

```{r}
auto_rate(sardine.rd, width = 600, method = "max")
```

##### Minimum rates:

We can similarly find the minimum rate over 10 minutes, and here we will save the output to object `x`:

```{r}
x <- auto_rate(sardine.rd, width = 600, method = "min")
print(x)
```

##### Additional ranked results:

`auto_rate()` objects contain all the results for the specified width (or in the case of `method = linear` all results for *all* widths). By default, the first ranked (i.e. most maximum or minimum) result is returned, but others and their locations can be examined using the `summary` command: 

```{r}
summary(x)
```

Particular ranked results can be examined or extracted via the `plot` and `summary` commands, using the `pos` (for position) operator:

```{r}
## Second most minimum rate detected
plot(x, pos = 2)
summary(x, pos = 2)
```

Note, the output objects of the `max` and `min` are essentially identical, the only difference being the results are ordered descending or ascending by rate, respectively. Therefore in this example, the *maximum* rate (i.e. *least* minimum) can be found by extracting the *last* result. 

```{r}
print(x, nrow(x$summary))
```

<!-- ### Interval rate -->

<!-- ```{r} -->
<!-- x <- auto_rate(sardine.rd, width = 1000, method = "interval") -->
<!-- print(x, 4)  # select the 4th interval result -->
<!-- ``` -->

<!-- ### Detecting and measuring linear regions -->

<!-- The function `auto_rate()` performs automatic detection of linear data by default: -->

<!-- ```{r} -->
<!-- x <- auto_rate(sardine.rd) -->
<!-- ``` -->

## Further processing
Saved `auto_rate()` objects can be passed to subsequent `respR` functions for further processing, such as `adjust_rate()` to correct for background respiration, or `convert_rate()` to convert to final O2 uptake rates. 

## Examples

Examples are available in the [reference section](https://januarharianto.github.io/respR/reference/index.html). You may also run `?auto_rate` in the R console to access some examples in the help file.


## References

Jones, M. C., Marron, J. S., & Sheather, S. J. (1996). A Brief Survey of Bandwidth Selection for Density Estimation. Journal of the American Statistical Association, 91(433), 401–407. doi:10.1080/01621459.1996.10476701

Olito, C., White, C. R., Marshall, D. J., & Barneche, D. R. (2017). Estimating monotonic rates from biological data using local linear regression. The Journal of Experimental Biology, jeb.148775-jeb.148775. doi:10.1242/jeb.148775

Raykar, V., & Duraiswami, R. (2006). Fast optimal bandwidth selection for kernel density estimation. In Proceedings of the Sixth SIAM International Conference on Data Mining (Vol. 2006). doi:10.1137/1.9781611972764.53

Sheather, S. J., & Jones, M. C. (1991). A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation. Journal of the Royal Statistical Society. Series B (Methodological), 53(3), 683–690.

Zivot, E., & Wang, J. (2006). Modeling Financial Time Series with S-PLUS (2nd ed.). New York: Springer-Verlag.
