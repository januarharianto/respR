---
title: "`auto_rate()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{automation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = F}
library(knitr) # load knitr to enable options
library(respR) # load respR

opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE, tidy = TRUE,
  highlight = TRUE, fig.width = 7.1, fig.height = 6)
```

## Introduction

The function `auto_rate()` uses rolling regression to automatically determine the *maximum*, *minimum*, or *most linear* sections of an oxygen time-series over a specific sampling window. In addition, to detect linear sections of the data, a kernel density estimate is performed on the rolling regression output and the kernel bandwidth used to detect linear regions of the data (Fig 1). The linear regions are re-analysed, again, using linear regression. The process is fully unsupervised and automated, requiring little to no input from the user.

<!-- ### Working in the Tidyverse -->

<!-- `respR` integrates nicely in the [Tidyverse](https://www.tidyverse.org/), specifically with `dplyr` functions e.g. `select()`, `filter()` and `mutate()`, and `magrittr` pipe operators ("`%>%`") to clearly express workflows in an organised sequence. For more information about using pipes in particular, see the ["Pipes" chapter](http://r4ds.had.co.nz/pipes.html) in the online R for Data Science book. -->

<!-- We load the data, `sardine.rd`, which contains approximately 2.1 hours (7,513 data points) of a single respirometry experiment. -->

<!-- ```{r} -->
<!-- library(respR) -->
<!-- data("sardine.rd") -->
<!-- ?sardine.rd # run this to see notes about the dataset -->
<!-- ``` -->


```{r, echo = F, fig.cap = "Fig 1. Methods used by `auto_rate()` to process data."}
library(DiagrammeR)
grViz("
digraph a_nice_graph {

# node definitions with substituted label text

node [fontname = Helvetica, shape = circle]        
rd [label = 'Raw Data']

node [fontname = Helvetica, shape = rectangle]        
rr [label = '1. Rolling linear regression']
kd [label = '2. Kernel density\nestimates']
re [label = '3. Bin resampling']
lr [label = '4. Linear regression']
ar [label = '5. Rank']
fi [label = '6. Filter']

node [fontname = Menlo, shape = none]
au [label = 'auto_rate()']
li [label = 'linear']
mm [label = 'max, min']
in [label = 'interval']

# edge definitions with the node IDs

rd -> au -> rr -> ar -> mm
rr -> kd -> re -> lr -> ar -> li
rr -> fi -> in

}")
```


### 1. Rolling linear regression

Using rolling linear regression techniques to automatically estimate *metabolic* parameters is uncommon in the current literature. Yet, the technique is an ubiquitous tool in financial statistitical analysis as it can accurately extract common parameters (max, min) and assess model stability with great efficiency. As respirometry data also relies on the key assumption that regression coefficients over a rolling window are constant over time (i.e. data is linear), rolling regression is an ideal analytical technique to detect common metabolic parameters and assess data linearty more accurately than commonly used empirical methods. 

A rolling regression performs *all* possible ordinary least-squares (OLS) linear regressions $(y = \beta X + \epsilon)$ of a specific width across a data series, and is expressed as: $$y_t(n) = X_t(n) \beta (n) + \epsilon_t(n), \ t = n,\ ...,\ T$$ where $n$ is the window of width $n < T$, $T$ is the total length of the dataset, $y_t(n)$ is the vector of observations on the response, $X_t(n)$ is the matrix of explanatory variables, $\beta (n)$ is a vector of regression parameters and $\epsilon_t(n)$ is a vector of error terms.

By default, `auto_rate()` performs the rolling regression at a fixed window, $n$, at one-fifth of the total length of the number of data samples (`by = "row"`) or total time elapsed (`by = "time"`). However, that value can be modified using the `width` argument as as a proportion, (`width = 0.1`) or as a fixed value  (`width = 600`).

If `auto_rate()` is used to estimate maximum, minimum or interval rates, no further transformation of the data is necessary. The regression coefficients are automatically filtered and ranked to produce the parameter of interest. Otherwise, additional computations are performed to estimate the most linear rate (see below). 

<!-- Any value less than 1 is automatically assumed to define a proportion, and is multiplied by the total number of samples (or total time elapsed) in the data before it is used.  -->


### 2. Kernel density estimates

<!-- The analysis of time-series data in both fields rely on the *same* key assumption that their parameters are constant over time (i.e. data is linear).  -->
<!-- When paired with kernel density estimation, the process of detecting linear stability and identifying the most linear (stable) sections of respirometry data can be automated. -->

A truly linear dataset should be reflected by a consistently stable rate over the rolling window after rolling regression has been performed. If the rate changes at some point during the sampling, then the variability should be detected in the rolling coefficients. Consequently, one may empirically select stable regions in the dataset by observing the rolling regression data and picking out regions that show stable values. 

We automate the above procedure by implementing Gaussian Kernel Density Estimation (KDE), a powerful technique that takes an unsupervised approach to estimate the underlying probability density function of the rolling regression dataset. KDE requires no assumption that the data is from a parametric family, and learns the shape of the density automatically. KDE can simply be expressed as: $$\hat{p}(x) = \frac{1}{nh^d}\sum_{i = 1}^{n} K \left(\frac{x - X_i}{h} \right)$$ where $p$ is the density function from an unknown distribution $P$ for $X_1,...,X_n$, $K$ is the kernel function and $h$ is the smoothing bandwidth. 

In R, KDE is implemented using the base `density()` function and implmented using the `bw.nrd0` kernel bandwidth selector, based on Silverman's "rule of thumb" (Silverman, 1986).

### 3. Bin resampling

### 4. Rank

### 5. Filter

## Examples

## Comparison to other methods and packages {#compare}

