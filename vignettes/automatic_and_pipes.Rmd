---
title: "Using `auto_rate()` to estimate common metrics"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{automation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = F}
library(knitr) # load knitr to enable options
library(respR) # load respR

opts_chunk$set(collapse = TRUE, comment = "#>", cache = FALSE, tidy = TRUE,
  highlight = TRUE, fig.width = 7.1, fig.height = 6)
```

## Introduction

The function `auto_rate()` can automatically determine the *maximum*, *minimum*, or *most linear* sections of an oxygen timeseries data. `auto_rate()` can also be used to determine multiple rates at fixed time *intervals*, which is useful when performing intermittent respirometry analyses at established time points over long periods.

<!-- ### Working in the Tidyverse -->

<!-- `respR` integrates nicely in the [Tidyverse](https://www.tidyverse.org/), specifically with `dplyr` functions e.g. `select()`, `filter()` and `mutate()`, and `magrittr` pipe operators ("`%>%`") to clearly express workflows in an organised sequence. For more information about using pipes in particular, see the ["Pipes" chapter](http://r4ds.had.co.nz/pipes.html) in the online R for Data Science book. -->

<!-- We load the data, `sardine.rd`, which contains approximately 2.1 hours (7,513 data points) of a single respirometry experiment. -->

<!-- ```{r} -->
<!-- library(respR) -->
<!-- data("sardine.rd") -->
<!-- ?sardine.rd # run this to see notes about the dataset -->
<!-- ``` -->

## How `auto_rate()` works

By default, `auto_rate()` detects the most linear section of a dataset, but it can also detect other parameters by altering the `method` argument, from `"linear"`, to `"max"`, `"min"` or `"interval"`. Regardless of the method used, the function always performs a rolling regression on the entire dataset before estimating the metric of interest. We can visualise the techniques used in the function with the following flowchart: 

```{r, echo = F}
library(DiagrammeR)
grViz("
digraph a_nice_graph {

# node definitions with substituted label text

node [fontname = Helvetica, shape = circle]        
rd [label = 'Raw Data']

node [fontname = Helvetica, shape = rectangle]        
rr [label = '1. Rolling linear regression']
kd [label = '2. Kernel density\nestimates']
ar [label = '4. Rank']
fi [label = '5. Filter']
re [label = '3. Bin resampling']

node [fontname = Menlo, shape = none]
li [label = 'linear']
mm [label = 'max, min']
in [label = 'interval']

# edge definitions with the node IDs

rd -> rr -> ar -> mm
rr -> kd -> re -> ar -> li
rr -> fi -> in

}")
```


### 1. Rolling linear regression

To our current knowledge, the use rolling linear regression to determine parameters such as maximum and minimum rates has not been documented in research involving metabolic rate (see [comparisons to other methods and packages](#compare)), but the technique is an ubiquitous tool in financial statistics. The analysis of time-series data in both fields rely on the *same* key assumption that their parameters are constant over time (i.e. data is linear). 

A rolling regression performs *all* possible ordinary least-squares (OLS) linear regressions $(y = \beta X + \epsilon)$ across the data expressed as: $$y_t(n) = X_t(n) \beta (n) + \epsilon_t(n), \ t = n,\ ...,\ T$$ where $n$ is the window of width $n < T$, the total length of the dataset, $y_t(n)$ is the vector of observations on the response, $X_t(n)$ is the matrix of explanatory variables, $\beta (n)$ is a vector of regression parameters and $\epsilon_t(n)$ is a vector of error terms.

By default, `auto_rate()` performs the rolling regression at a fixed window, $n$, that is one-fifth of the total length of the number of data points (`by = "row"`) or total time elapsed (`by = "time"`). However, that value can be modified using the `width` argument (e.g. as a proportion, `width = 0.1`; as a fixed value, `width = 600`).


<!-- Any value less than 1 is automatically assumed to define a proportion, and is multiplied by the total number of samples (or total time elapsed) in the data before it is used.  -->


### 2. Kernel density estimates



We take advantage of the fact that a truly linear dataset should be reflected by a consistently stable (i.e. flat) rate over the rolling window. If the rate changes at some point during the sampling, then the variability should be detected in the rolling estimates. Consequently, one may empirically select stable regions in the dataset by observing the rolling regression data and selecting regions that are stable. 

We standardise and automate this process using Gaussian Kernel Density Estimation (KDE), a powerful technique that takes an unsupervised approach to detect linear regions of a dataset. Because KDE is a non-parametric estimator, it determines the density of the most common 

A linear 


### 3. Bin resampling

### 4. Rank

### 5. Filter

## Examples

## Comparison to other methods and packages {#compare}

